---
layout: post
title: "Fantastic Kedro and How to Use It"
date: 2021-02-08 14:43:23 +0700
categories:
  - Big Data
tags:
  - kedro
  - big-data
  - data-engineering
  - python
---



**Imagine this condition**: You do some data analytics stuff on [Jupyter Notebook][jupyter]. You write a line of code, add a new cell, import [pandas][pandas], read CSV, transform the values, then write 400 other cells before you get the result you want and save it as CSV.

A week later, you want to produce that very same result, but you completely forgot which piece of code you should run first, and which _cell_ you should NOT run. You will also need to press _enter_ 400-ish times before you get the result you are looking for, will take you like a half-day just to reproduce the result.

Also, imagine this: The input data which you need are being generated by someone else’s _Jupyter Notebooks_. You would need to ask them to generate the data first, which they need to run their error-prone notebooks. Another half-day wasted.

You might be wondering if there is a tool that makes your scripts reproducible, maintainable and modular; a tool that helps you to do separation of concerns and versioning; and a tool that helps you to deliver real-world ML applications; your life would be easier.

## Enter Kedro

[Kedro][kedro] is an open-source Python framework that makes it easy to build robust and scalable data pipelines by providing uniform project templates, data abstraction, configuration, and pipeline assembly. By using kedro, we write our data analytics/machine learning scripts with software engineering principles in mind.

We would need to do some installations & setups before we can use the Kedro framework. We are not going into details about how to install kedro, you can find it [here][kedro-prerequisites].

To create a new kedro project, we can run `kedro new` on the CLI, then we need to input the project name, repository name, and python package name. Let us use `bmi` for all the names.

```
$ kedro new
Project Name:
=============
Please enter a human readable name for your new project.
Spaces and punctuation are allowed.
 [New Kedro Project]: bmi
Repository Name:
================
Please enter a directory name for your new project repository.
Alphanumeric characters, hyphens and underscores are allowed.
Lowercase is recommended.
 [new-kedro-project]: bmi
Python Package Name:
====================
Please enter a valid Python package name for your project package.
Alphanumeric characters and underscores are allowed.
Lowercase is recommended. Package name must start with a letter or underscore.
 [new_kedro_project]: bmi
Generate Example Pipeline:
==========================
Do you want to generate an example pipeline in your project?
Good for first-time users. (default=N)
 [y/N]: N
Change directory to the project generated in /home/user/bmi
```

This is how the kedro project usually structured:

```
bmi                     # Parent directory of the template
    ├── conf            # Project configuration files
    ├── data            # Local project data
    ├── docs            # Project documentation
    ├── logs            # Project output logs
    ├── notebooks       # Project related Jupyter notebooks
    ├── README.md       # Project README
    ├── setup.cfg       # Configuration options for `pytest`
    └── src             # Project source code
```

There are several important concepts in kedro, but we will be focusing on the 3 most important concepts: _DataCatalog_, _Node_, and _Pipeline_.

### DataCatalog

DataCatalog is the registry of all data sources that the project can use. DataCatalog is a powerful concept. We can find all data sources & sinks in one place, as opposed to _Jupyter Notebook_ or plain python script that the data definition is scattered everywhere.

The DataCatalog is stored in the yaml file named `catalog.yml` under `conf/bmi/` folder. For example, if we have 3 CSVs as the data input/output, we can define all of them in one file:

```yaml
freshman_bmi:
  type: pandas.CSVDataSet
  filepath: data/01_raw/weight_bmi.csv
  load_args:
    sep: ','
  save_args:
    index: False
    decimal: .

freshman_with_height:
  type: pandas.CSVDataSet
  filepath: data/02_primary/weight_bmi_height.csv
  load_args:
    sep: ','
  save_args:
    index: False
    decimal: .

freshman_bmi_summary:
  type: pandas.CSVDataSet
  filepath: data/03_summary/weight_bmi_height_summary.csv
  load_args:
    sep: ','
  save_args:
    index: False
    decimal: .
```

The topmost level of the yaml key is the catalog name (e.g `freshman_bmi`), this name will be used by _Node_ later as a reference to the input/output data. We should also define the type and file path of the data.

In the example we use [`pandas.CSVDataSet`][pandas-csv] type, but we can also use other types like [`SparkDataSet`][spark-kedro] , [`pandas.ExcelDataSet`][pandas-excel] , [`pandas.SQLQueryDataSet`][pandas-sql], and many more. You can find the completed DataSet type [here][kedro-dataset].

We can also define the arguments for load/save, like which CSV separator we use, or whether we overwrite or append the file, and so on.

### Node
Node in kedro is a python function wrapper that names the inputs and outputs of that function. We can link one Node to another by setting the output of one node as the input to another node.

For example, let us say we have 2 tasks which will:
- Calculate the height of a person based on the weight & BMI information, and save it to CSV
- Calculate the average weight, height, and BMI for each gender, and save it to CSV

We should first create a function for each of them:

```python
import pandas as pd
import numpy as np


def calculate_height(df):
  df["height"] = np.sqrt(df["weight"] / df["bmi"])
  return df


def calculate_avg_by_gender(df):
  df = df.groupby('gender').mean()
  return df
```

The node contains several parameters:
- func: A function that corresponds to the node logic.
- inputs: The name or the list of the names of variables used as inputs to the function. We can put the catalog name that we defined on catalog.yml here.
- outputs: The name or the list of the names of variables used as outputs to the function. We can put the catalog name that we defined on catalog.yml here.
- name: Optional node name to be used when displaying the node in logs or any other visualisations.
- tags: Optional set of tags to be applied to the node.

In this example, we would then need to create 2 nodes:

```python
import pandas as pd
import numpy as np
from kedro.pipeline import node


def calculate_height(df):
    df["height"] = np.sqrt(df["weight"] / df["bmi"])
    return df


def calculate_avg_by_gender(df):
    df = df.groupby('gender').mean()
    return df

nodes = [
    node(
        func=calculate_height,
        inputs="freshman_bmi",
        outputs="freshman_with_height",
        name="calculate_height",
    ),
    node(
        func=calculate_avg_by_gender,
        inputs="freshman_with_height",
        outputs="freshman_bmi_summary",
        name="calculate_avg_by_gender",
    ),
]
```

[jupyter]: https://jupyter.org/
[pandas]: https://pandas.pydata.org/
[kedro]: https://github.com/quantumblacklabs/kedro
[kedro-prerequisites]: https://kedro.readthedocs.io/en/stable/02_get_started/01_prerequisites.html
[pandas-csv]: https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.pandas.CSVDataSet.html#kedro.extras.datasets.pandas.CSVDataSet
[spark-kedro]: https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.spark.SparkDataSet.html#kedro.extras.datasets.spark.SparkDataSet
[pandas-excel]: https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.pandas.ExcelDataSet.html#kedro.extras.datasets.pandas.ExcelDataSet
[pandas-sql]: https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.pandas.SQLQueryDataSet.html#kedro.extras.datasets.pandas.SQLQueryDataSet
[kedro-dataset]: https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html#data-sets

